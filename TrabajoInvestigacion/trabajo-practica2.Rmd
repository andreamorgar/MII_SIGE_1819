---
title: "Trabajo práctica 2"
output: html_notebook
---

```{r}
library(tidyverse)
library(tm)
library(wesanderson)
library(rword2vec) #install_github("mukul13/rword2vec")
```


## Ideas del dataset

```{r}
data.train <- read.csv("datos/train.csv")
head(data.train, 5)
# La columna de texto que nos interesa es "Description"
```

**¿La columna que nos interesa predecir es "AdoptionSpeed"? ¿O cuál?**

```{r}
data.train %>% 
  count(AdoptionSpeed) %>%
    ggplot(aes(x = factor(AdoptionSpeed), y = n, fill = factor(AdoptionSpeed))) +
        geom_col(color = "black") +
        geom_text(aes(label = n), position = position_stack(vjust = 0.5)) +
        scale_fill_brewer(palette="Paired") +
        labs(x = "Puntuación del medicamento", y = "Tiempo que se tarda en adoptar") +
        ggtitle("Adopciones") +
        theme(legend.position = "none", plot.title = element_text(hjust = 0.5, face = "bold", size=40))

# Sino me equivoco el 0 es el mínimo tiempo en adoptar y el 4 es el máximo tiempo en adoptar
```

## Creación del corpus

```{r}
# Nos quedamos con la única columna del dataset que nos interesa. 
# Necesitamos obtenerla en forma de vector, y no como un dataframe de una columna, 
# por lo que usamos as.vector para hacer la conversión
data.train.description <- data.train$Description
data.train.description <- as.vector(data.train.description)

# Lo convertimos en la estructura de documento, y lo guardamos ya en el corpus 
# que lo vamos a utilizar
data.train.description.corpus <- (VectorSource(data.train.description))

# Creamos el propio corpus
data.train.description.corpus <- Corpus(data.train.description.corpus)
```

```{r}
# Mostrar el contenido por pantalla
data.train.description.corpus[[4]]$content
data.train.description.corpus[[7]]$content
```

## Conversión de las mayúsculas en minúsculas

```{r}
data.train.description.corpus <- tm_map(data.train.description.corpus, content_transformer(tolower))
```

```{r}
# Mostrar el contenido por pantalla
data.train.description.corpus[[4]]$content
data.train.description.corpus[[7]]$content
```

## Eliminar signos de puntuación

```{r}
data.train.description.corpus <- tm_map(data.train.description.corpus, content_transformer(removePunctuation))
```

```{r}
# Mostrar el contenido por pantalla
data.train.description.corpus[[4]]$content
data.train.description.corpus[[7]]$content
```


## Agrupación de sinónimos

```{r}
# Obtenemos su matriz de términos
matrix.train.description.corpus <- TermDocumentMatrix(data.train.description.corpus)

# No tenemos los datos en la matriz que buscamos, sino en un vector
# por tanto, lo convertimos en matriz
matrix.train.description.corpus <- as.matrix(matrix.train.description.corpus)

# Sumamos las filas para obtener la frecuencia de una palabra en benefitsReview
matrix.train.description.corpus <- rowSums(matrix.train.description.corpus)

# Ordenamos de mayor a menor los términos
terms.frecuency.train.description.corpus <- sort(matrix.train.description.corpus, decreasing = TRUE)
terms.frecuency.train.description.corpus <- terms.frecuency.train.description.corpus[1:length(data.train.description.corpus)]
terms.frecuency.train.description.corpus
```

```{r}
graph.terms.frecuency.train.description.corpus <- as.matrix(terms.frecuency.train.description.corpus)
barplot(graph.terms.frecuency.train.description.corpus[1:4,],  xlab="Términos", ylab="Número de frecuencia",
        col=wes_palette(n=4, name="Zissou1"))
```

Una vez que tenemos los términos con mayor frecuencia en nuestras columnas y su frecuencia asociada, pasamos a matriz dichos datos, con el fin de obtener únicamente las palabras y descartar su frecuencia asociada.

```{r}
# Convertimos a matriz "terms.frecuency.train.description.corpus"
terms.frecuency.train.description.corpus <- as.matrix(terms.frecuency.train.description.corpus)
terms.frecuency.train.description.corpus

# Me quedo solo con los términos
terms.train.description.corpus <- rownames(terms.frecuency.train.description.corpus)
terms.train.description.corpus
```

Como ya sabemos las palabras a usar, es decir, los términos que más se repiten, procedemos a la agrupación por sinónimos. En donde, mediante la función *distance(...)* de la librería *rword2vec*, obtendremos todas palabras más similares de nuestro conjunto. En nuestro caso nos vamos a quedar con las 2 primeras. A continuación, se muestran los pasos seguidos.

1. Escribir un fichero los datos asociados a dicha columna.
2. Entrenar los datos del fichero, con el fin de obtener los vectores de palabras que nos darán los palabras más similares.
3. Obtener para cada término del documento, la distancia con los térmios del ficheros, quedándonos con las de mayor frecuencia.
4. Guardar en un fichero dichas distancias.

```{r}
# PETA HASTA QUE NO SE LIMPIE MUCHO MÁS

# Escribo en un fichero la columna "description"
# write.table(data.train$Description, "description.txt", sep = "\t", quote = F, row.names = F)

# Entreno los datos del texto para obtener los vectores de palabras
# model.train.description = word2vec(train_file = "description.txt", output_file = "description.bin", binary=1)

#dist_terms_benefits_train_corpus = c()
# Obtengo la distancia de las 100 palabras con mayor frecuencia
#for (i in 1:length(terms_benefits_train_corpus)){ # calculamos la distancia de la palabra a sus sinónimos
#  dist_terms_benefits_train_corpus[i] = distance(file_name = "benefitsTrainReview.bin", 
#                                                 search_word = terms_benefits_train_corpus[i], num = 2)
#}
# guardamos en un fichero
#list.save(dist_terms_benefits_train_corpus, 'datos/ficheros-sinonimos/benefits/dist_terms_benefits_train_corpus.RData')
#dist_terms_benefits_train_corpus_new <- list.load('datos/ficheros-sinonimos/benefits/dist_terms_benefits_train_corpus.RData')
```






